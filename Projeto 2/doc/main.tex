\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[portuguese]{babel}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{xcolor}
\usepackage{url}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{listings}

\lstset{ 
    basicstyle=\ttfamily\small, % Fonte monoespaçada
    numbers=left, % Numeração das linhas
    numberstyle=\tiny\color{gray}, keywordstyle=\color{blue}, % Cor das palavras-chave
    commentstyle=\color{green!50!black}, % Cor dos comentários
    stringstyle=\color{red}, % Cor das strings
    breaklines=true, % Quebra automática de linha
    tabsize=2 % Tamanho do tab
}

% Configurações de página
\geometry{margin=2.5cm}
\onehalfspacing{}

% Configurações do hyperref
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=blue
}

\begin{document}
    \begin{titlepage}
        \centering
        \vspace*{2cm}

        {\huge\bfseries Projeto 2 --- DGEMM Sequencial e Paralela com MPI\par}
        \vspace{1.5cm}

        {\Large \textbf{Autores:}\\ Italo Santana Seara\\ Wilson Santos Silva Filho\\ }

        \vspace{2cm}

        {\large \textbf{Disciplina:} DEC107 — Processamento Paralelo\\ \textbf{Professor:} Esbel Tomas Valero Orellana\\ \textbf{Data:} \today\\ }

        \vfill
    \end{titlepage}

    \tableofcontents

    \newpage
    \section{Introdução}\label{sec:introducao}

    A multiplicação de matrizes é uma operação fundamental em diversas áreas da computação científica e engenharia, presente em aplicações que vão desde álgebra linear numérica até aprendizagem de máquina e simulações físicas. Neste projeto, focamos na versão de precisão dupla da multiplicação geral de matrizes (DGEMM), cujo problema pode ser formalizado como: dado $A\in\mathbb{R}^{m\times k}$ e $B\in\mathbb{R}^{k\times n}$, calcular $C=AB$, onde
    
    \[
        C_{ij}=\sum_{t=1}^{k} A_{it}\,B_{tj}, \qquad 1 \le i \le m, \;1 \le j \le n.
    \]

    A complexidade aritmética desta operação cresce como $\mathcal{O}(mkn)$ e, para matrizes quadradas de ordem $N$, como $\mathcal{O}(N^{3})$, o que torna essencial a exploração de paralelismo para resolver instâncias de grande porte de forma eficiente.

    No Projeto 1 foram implementadas versões sequenciais e paralelas (por memória compartilhada, com OpenMP) da DGEMM.\@ No presente Projeto 2, damos continuidade ao estudo do paralelismo ao migrar para o modelo de memória distribuída, implementando a DGEMM com o padrão MPI (Message Passing Interface). O objetivo central é projetar e avaliar uma implementação paralela distribuída que explore comunicação por troca de mensagens entre processos, comparando sua corretude e desempenho com as versões sequencial e com OpenMP.\@

    Este projeto tem como objetivos específicos:
    \begin{itemize}
        \item Implementar uma versão paralela distribuída de DGEMM utilizando apenas rotinas básicas do MPI (por exemplo, \texttt{MPI\_Init}, \texttt{MPI\_Comm\_size}, \texttt{MPI\_Comm\_rank}, \texttt{MPI\_Send}, \texttt{MPI\_Recv}, \texttt{MPI\_Bcast}, \texttt{MPI\_Scatter}, \texttt{MPI\_Gather}).
        \item Validar a corretude numérica da implementação MPI comparando-a com a versão sequencial e calculando a diferença relativa máxima entre os elementos do resultado, usando estratégias que evitem divisão por zero.
        \item Medir e analisar desempenho (tempo total, \emph{speedup} e eficiência) em várias configurações de processos e tamanhos de matrizes, e comparar os resultados com os obtidos em ambiente de memória compartilhada (OpenMP).
        \item Discutir trade-offs entre modelos de memória (compartilhada vs distribuída), identificando gargalos relacionados à comunicação (latência e largura de banda) e ao balanceamento de carga.
    \end{itemize}

    Para atingir esses objetivos, a estratégia adotada contempla a decomposição das matrizes por blocos (linhas ou colunas) --- detalhada na seção de metodologia --- e a utilização de vetores unidimensionais (\textit{C-style arrays}) para o armazenamento compacto das matrizes. A análise de desempenho considera tempos médios sobre múltiplas execuções e avalia tamanhos de problema representativos (por exemplo, ordens 512, 1024, 2048 e 4096), assim como configurações com 1, 2, 4 e, quando possível, 8 processos.

    Este relatório está organizado da seguinte forma: na Seção~\ref{sec:metodologia} descreve-se a implementação sequencial e paralela, as rotinas MPI utilizadas e a metodologia de medição; a Seção~\ref{sec:resultados} apresenta testes de corretude, tabelas e gráficos de desempenho e os cálculos de \emph{speedup} e eficiência; a Seção~\ref{sec:discussao} analisa os resultados e discute limitações; por fim, a Seção~\ref{sec:conclusao} sintetiza os aprendizados e sugere melhorias futuras.

    \newpage
    \section{Metodologia}\label{sec:metodologia}

    Nesta seção, detalhamos a implementação da multiplicação de matrizes DGEMM em suas versões sequencial e paralela utilizando MPI, bem como a metodologia adotada para medir desempenho e validar corretude.

    \subsection{Implementação Sequencial}

    A implementação sequencial da DGEMM utiliza a versão otimizada do Projeto 1, que emprega blocos para melhorar a localidade de dados. As matrizes são armazenadas em vetores unidimensionais (\textit{C-style arrays}) para garantir um acesso eficiente à memória. A função principal responsável pela multiplicação é apresentada no repositório do projeto.\footnotemark[1]

    Esta implementação serve como referência para comparação de desempenho com a versão paralela.

    \subsection{Implementação Paralela com OpenMP}

    A versão paralela com OpenMP, desenvolvida no Projeto 1, utiliza diretivas de paralelismo para distribuir o trabalho entre múltiplas threads em um ambiente de memória compartilhada. A decomposição por blocos é mantida, e a paralelização é aplicada principalmente nos loops externos da multiplicação de matrizes. 
    
    Esta implementação também está disponível no repositório do projeto.\footnotemark[1]

    \subsection{Implementação Paralela com MPI}

    A versão paralela da DGEMM foi desenvolvida utilizando o padrão MPI, que permite a comunicação entre processos em um ambiente de memória distribuída. A estratégia adotada envolve a decomposição das matrizes por blocos, onde cada processo é responsável por calcular uma parte da matriz resultado $C$. A seguir, descrevemos os principais passos da implementação:
    
    \begin{itemize}
        \item Inicialização do ambiente MPI utilizando \texttt{MPI\_Init}.
        \item Determinação do número total de processos e do identificador de cada processo com \texttt{MPI\_Comm\_size} e \texttt{MPI\_Comm\_rank}.
        \item Distribuição das matrizes $A$ e $B$ entre os processos utilizando \texttt{MPI\_Scatter} para $A$ e \texttt{MPI\_Bcast} para $B$.
        \item Cada processo calcula sua parte da matriz $C$ localmente.
        \item Coleta dos resultados parciais de $C$ utilizando \texttt{MPI\_Gather} (apenas o processo raiz (rank 0) retorna o resultado final).
        \item Finalização do ambiente MPI com \texttt{MPI\_Finalize}.
    \end{itemize}

    A implementação completa está disponível no repositório do projeto.\footnotemark[1]

    \footnotetext[1]{Link para as implementações: \url{https://github.com/italoseara/DEC107/blob/main/Projeto 2/src/dgemm.c}}

    \subsection{Validação de Corretude}

    A corretude da implementação sequencial e com OpenMP não foram validadas no relatório do Projeto 1, portanto, desta vez, a versão sequencial foi validada comparando os resultados obtidos com a função \texttt{cblas\_dgemm} da biblioteca BLAS (Basic Linear Algebra Subprograms), que é amplamente reconhecida por sua eficiência e precisão numérica. A versão com OpenMP foi validada comparando seus resultados com os da versão sequencial, da mesma forma que a versão MPI.\@

    A corretude da implementação paralela com MPI foi realizada comparando o resultado obtido com o da versão sequencial, calculando a diferença relativa máxima entre os elementos das matrizes resultantes, utilizando uma abordagem que evita divisão por zero. A fórmula utilizada foi:

    \[
        \Delta = \max_{i,j} \frac{|C_{\text{seq}}(i, j) - C_{\text{mpi}}(i, j)|}{|C_{\text{seq}}(i, j)| + \epsilon}, \quad \epsilon = 10^{-12}
    \]

    Considerando corretas implementações cuja diferença relativa máxima seja inferior a $10^{-9}$.

    \subsection{Descrição do hardware utilizado nos testes}

    Os testes foram realizados em uma máquina com as seguintes especificações:
    \begin{itemize}
        \item \textbf{Processador:} Ryzen 7 5700G (8 núcleos)
        \item \textbf{Memória RAM:} 32 GB
        \item \textbf{Sistema Operacional:} Windows 11 + WSL 2.4.10.0 (Ubuntu 24.04.2 LTS)
        \item \textbf{Compilador:} GCC 13.3.0
    \end{itemize}

    \subsection{Métricas utilizadas para avaliação}
	
    Usaremos as seguintes métricas para definir e comparar as implementações:

    \subsubsection{Tempo de Execução}

    Medido em segundos, é o tempo total gasto para completar a multiplicação das matrizes. Não inclui o tempo de inicialização ou finalização do programa, apenas o tempo gasto na execução da função de multiplicação.
    \[
        T = T_{end} - T_{start}
    \]
    Onde:
    \begin{itemize}
        \item $T$ é o tempo de execução.
        \item $T_{start}$ é o tempo registrado no início da execução da função.
        \item $T_{end}$ é o tempo registrado ao final da execução da função.
    \end{itemize}

    O tempo total de execução será medido utilizando a função \texttt{MPI\_Wtime()} para a versão MPI, e \texttt{omp\_get\_wtime()} para a versão OpenMP e para a versão sequencial.

    Foi utilizado um script em python para automatizar a execução dos testes e coletar os tempos. O script executa cada versão múltiplas vezes com diferentes entradas para obter uma média dos tempos, calcula as métricas de desempenho e gera um relatório com os resultados, que foram utilizados para criar os gráficos apresentados na seção de resultados.

    \subsubsection{Speedup}
	Mede o ganho de desempenho da versão paralela em relação à sequencial.
	\[
		S_{p} = \frac{T_{s}}{T_{p}}
	\]
	Onde:
	\begin{itemize}
		\item $S_{p}$ é o speedup com $p$ processadores/threads.

		\item $T_{s}$ é o tempo de execução da versão sequencial.

		\item $T_{p}$ é o tempo de execução da versão paralela com $p$ processadores/threads.
	\end{itemize}

	\subsubsection{Eficiência}
    Mede quão bem os recursos de processamento estão sendo utilizados pela versão paralela. Onde 1 é a eficiência ideal (100\%).
	\[
		E_{p} = \frac{S_{p}}{p}= \frac{T_{s}}{p \cdot T_{p}}
	\]
	Onde:
	\begin{itemize}
		\item $E_{p}$ é a eficiência com $p$ processadores/threads. Varia entre 0 e 1.

		\item $S_{p}$ é o speedup com $p$ processadores/threads.

        \item $p$ é o número de processadores/threads.

        \item $T_{s}$ é o tempo de execução da versão sequencial.

        \item $T_{p}$ é o tempo de execução da versão paralela com $p$ processadores/threads.
	\end{itemize}

    \newpage
    \section{Resultados}\label{sec:resultados}

    % Conteúdo de resultados (tabelas, gráficos, análises)

    \newpage
    \section{Discussão}\label{sec:discussao}

    % Discussão dos resultados, limitações e análise

    \newpage
    \section{Conclusão}\label{sec:conclusao}

    % Síntese dos aprendizados e propostas futuras

\end{document}
