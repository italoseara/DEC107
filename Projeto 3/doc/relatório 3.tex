\documentclass[12pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[portuguese]{babel}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{url}
\usepackage{hyperref}
\usepackage{setspace}
\usepackage{listings}
\usepackage{float}
\usepackage{graphicx}
\lstset{ 
	basicstyle=\ttfamily\small, % Fonte monoespaçada
	numbers=left, % Numeração das linhas
	numberstyle=\tiny\color{gray}, keywordstyle=\color{blue}, % Cor das palavras-chave
	commentstyle=\color{green!50!black}, % Cor dos comentários
	stringstyle=\color{red}, % Cor das strings
	breaklines=true, % Quebra automática de linha
	tabsize=2 % Tamanho do tab
}

% Configurações de página
\geometry{margin=2.5cm}
\onehalfspacing{}

% Configurações do hyperref
\hypersetup{
	colorlinks=true,
	linkcolor=black,
	filecolor=magenta,
	urlcolor=blue,
	citecolor=blue
}

\begin{document}
	\begin{titlepage}
		\centering
		\vspace*{2cm}
		
		{\huge\bfseries Projeto 3 --- DGEMM Sequencial e Paralela para processamento em GPGPUs com CUDA\par}
		\vspace{1.5cm}
		
		{\Large \textbf{Autores:}\\ Italo Santana Seara\\ Wilson Santos Silva Filho\\ }
		
		\vspace{2cm}
		
		{\large \textbf{Disciplina:} DEC107 — Processamento Paralelo\\ \textbf{Professor:} Esbel Tomas Valero Orellana\\ \textbf{Data:} \today\\ }
		
		\vfill
	\end{titlepage}
	
	\tableofcontents
	
	\newpage
	\section{Introdução}\label{sec:introducao}
	
	A multiplicação de matrizes é uma operação fundamental em diversas áreas da computação científica e engenharia, presente em aplicações que vão desde álgebra linear numérica até aprendizagem de máquina e simulações físicas. Neste projeto, focamos na versão de precisão dupla da multiplicação geral de matrizes (DGEMM), cujo problema pode ser formalizado como: dado $A\in\mathbb{R}^{m\times k}$ e $B\in\mathbb{R}^{k\times n}$, calcular $C=AB$, onde
	
	\[
	C_{ij}=\sum_{t=1}^{k} A_{it}\,B_{tj}, \qquad 1 \le i \le m, \;1 \le j \le n.
	\]
	
	A complexidade aritmética desta operação cresce como $\mathcal{O}(mkn)$ e, para matrizes quadradas de ordem $N$, como $\mathcal{O}(N^{3})$, o que torna essencial a exploração de paralelismo para resolver instâncias de grande porte de forma eficiente.
	
	No Projeto 1 foram implementadas versões sequenciais e paralelas (por memória compartilhada, com OpenMP) da DGEMM.\@ No Projeto 2, foi implementado a versão com o MPI, onde o estudo do paralelismo migrando o algorítimo para modelo de memória distribuída. No presente projeto 3, daremos continuidade ao estudo de processamento paralelo implementando a DGEMM em GPGPUs (General-Purpose computing on Graphics Processing Units) utilizando o alto poder de processamento desses \textit{devices}.\@
	
	Este projeto tem como objetivos específicos:
	\begin{itemize}
		\item Implementar uma versão paralela de DGEMM utilizando a programação de GPGPUs para Nvidia (CUDA) e utilizar algumas rotinas  (por exemplo, \texttt{cudaMemoryCopy}, implementar estrutura hierárquica de memória (\texttt{\_\_global\_\_, \_\_device\_\_, \_\_host\_\_ e \_\_shered\_\_}, )).
		\item Comparar o resultado da versão CUDA com o resultado da versão sequencial, calcular a diferença relativa máxima.
		\item Medir e analisar desempenho (tempo total, \emph{speedup} e eficiência) em várias configurações de blocos e threds e tamanhos de matrizes, e comparar os resultados com os obtidos em ambiente de memória compartilhada (OpenMP, MPI).
		\item Comparar os resultados com os projetos 1 e 2, apresentando discussões sobre o ganho obtido com o uso da GPU e a diferença de escalabilidade entre os modelos de paralelismo
	\end{itemize}
	
	
	\newpage
	\section{Metodologia}\label{sec:metodologia}
	
	Nesta seção, detalhamos a implementação da multiplicação de matrizes DGEMM em suas versões sequencial e paralela utilizando programação CUDA, bem como a metodologia adotada para medir desempenho e validar diferença relativa máxima.
	
	\subsection{Implementação Sequencial}
	
	A implementação sequencial da DGEMM utiliza a versão otimizada do Projeto 1, que emprega blocos para melhorar a localidade de dados. As matrizes são armazenadas em vetores unidimensionais (\textit{row-major order}) para garantir um acesso eficiente à memória. A função principal responsável pela multiplicação é apresentada no repositório do projeto.\footnotemark[1]
	
	Esta implementação serve como referência para comparação de desempenho com a versão paralela da GPU com CUDA.
	
	\subsection{Implementação Paralela com OpenMP}
	
	A versão paralela com OpenMP, desenvolvida no Projeto 1, utiliza diretivas de paralelismo para distribuir o trabalho entre múltiplas threads em um ambiente de memória compartilhada. A decomposição por blocos é mantida, e a paralelização é aplicada principalmente nos loops externos da multiplicação de matrizes. 
	
	Esta implementação também está disponível no repositório do projeto.\footnotemark[1]
	
	\subsection{Implementação Paralela com MPI}
	
	A versão paralela da DGEMM desenvolvida utilizando o padrão MPI no Projeto 2, que permite a comunicação entre processos em um ambiente de memória distribuída, utilizou-se de estratégias que envolve e adota a decomposição das matrizes por blocos, onde cada processo é responsável por calcular uma parte da matriz resultado $C$. Esta implementação também está disponível no repositório do projeto.
	
	\subsection{Implementação Paralela para GPGPUs com CUDA}
	
	A versão paralela da DGEMM foi desenvolvida utilizando a programação para GPGPUs da Nvidia, utilizando CUDA. A GPU possui milhares de núcleos, organizar quem faz o quê é crucial. As GPUS usam um sistema de coordenadas onde a Thread: A menor unidade de execução. Cada thread roda o Kernel. O Block (Bloco): Um grupo de threads que podem compartilhar memória rápida (Shared Memory) e se sincronizar e a Grid (Grade): O conjunto total de blocos que executam o kernel.
	A implementação completa está disponível no repositório do projeto.\footnotemark[1]
	
	\subsubsection{Organização de Threads e Blocos}
	
	O código utiliza uma estratégia de decomposição de domínio em 2D, mapeando threads diretamente para as coordenadas da matriz de resultado $C$. 
	Estrutura de Bloco (dim3 block): O kernel é configurado com blocos quadrados de tamanho tile x tile (por padrão, $32 \times 32$, totalizando 1024 threads por bloco, que é o máximo permitido na maioria das arquiteturas CUDA).
	Estrutura de Grade (dim3 grid): A grade é calculada para cobrir as dimensões da matriz $C$ ($M$ linhas por $N$ colunas). O número de blocos é arredondado para cima ((N + tile - 1) / tile) para garantir que matrizes cujas dimensões não sejam múltiplos de 32 sejam processadas corretamente.
	Mapeamento: threadIdx.x mapeia para a coluna dentro do bloco (e do tile).threadIdx.y mapeia para a linha dentro do bloco.A coordenada global $(row, col)$ é calculada como:$$row = blockIdx.y \times blockDim.y + threadIdx.y$$$$col = blockIdx.x \times blockDim.x + threadIdx.x$$
	
	\subsubsection{Uso de Memória Compartilhada \texttt{\_\_shared\_\_} e Hierarquia de Memória}
	A diferença crítica entre as funções \texttt{dgemm\_kernel\_basic} e \texttt{dgemm\_kerne}l é o uso da hierarquia de memória para contornar o gargalo de largura de banda da memória global (DRAM).
	A Otimização via Tiling: O \texttt{dgemm\_kernel} implementa a técnica de Tiling (ladrilhamento).
	À Declaração: \texttt{extern \_\_shared\_\_ double smem[];} aloca memória rápida on-chip (L1/Shared), acessível por todas as threads do mesmo bloco.
	Utilizamos o uso de carregamento cooperativo: O código divide as matrizes $A$ e $B$ em sub-blocos (tiles) quadrados de tamanho $T \times T$. Todas as threads do bloco colaboram para carregar um tile de $A$ e um tile de $B$ da memória global (lenta) para a memória compartilhada (rápida). O comando \_\_syncthreads() garante que todo o tile esteja carregado na memória compartilhada antes que qualquer thread comece a calcular.
	Do Cálculo: Uma vez que os dados estão na smem, as threads calculam o produto escalar parcial lendo apenas da memória compartilhada. \\
	Isso causa um impacto: Em vez de fazer $2 \times K$ leituras da memória global por thread (como no kernel básico), cada elemento é lido da memória global apenas uma vez por tile e reutilizado $T$ vezes. Isso reduz o tráfego de memória global por um fator de $T$ (neste caso, 32 vezes menos acesso à RAM da GPU).
	
	\subsubsection{Sobreposição de Computação e Comunicação}
	Se analisarmos algumas funções como a \texttt{dgemm\_paralell\_cuda} podemos observar um fluxo:
	\\
	\texttt{cudaMemcpy(..., cudaMemcpyHostToDevice)} onde realiza-se a Cópia H->D\\
	\texttt{$dgemm\_kernel<<<...>>> $} (Execução do Kernel)
	\texttt{cudaMemcpy(..., cudaMemcpyDeviceToHost)}  (Cópia D->H)\\
	Percebemos que não há sobreposição de computação e comunicação pois: As chamadas \texttt{cudaMemcpy} por padrão são bloqueantes (ou seja, síncronas em relação ao host ou seria lizadas nos stream padrão )
	Então, a GPU fica ociosa durante a transferencia de dados via PCLe, e o PCLe fica ocioso durante o calculo no Kernel. \\
	Para implementações futuras: Para matrizes muito grandes, o tempo de transferência pode ser significativo, o que deveria ser usado com alicerce e as funções de CUDA Streams e AsyncMemcpy para que a GPU processe um pedaço da matriz enquanto o outro esta sendo transferido.
	
		\footnotetext[1]{Link para as implementações: \url{https://github.com/italoseara/DEC107/blob/main/Projeto 3/src/dgemm_cuda.cu}}
		
	
	\subsection{Diferença relativa máxima}
	
	Vamos validar a versão sequencial comparando os resultados obtidos com a função \texttt{...}. A implementação sequencial (rodando na CPU) serve como a referência (ground truth). Ela é, por definição, a maneira mais simples e verificável de calcular o resultado (neste caso, a multiplicação de matrizes).\@
	
	REVISAR A corretude da implementação paralela com CUDA foi realizada comparando o resultado obtido com o da versão sequencial, calculando a diferença relativa máxima entre os elementos das matrizes resultantes, utilizando uma abordagem que evita divisão por zero. A fórmula utilizada foi:
	\begin{equation}
		\Delta = \max_{i,j} \frac{|C_{\text{seq}}(i,j) - C_{\text{cuda}}(i,j)|}{|C_{\text{seq}}(i,j)| + \epsilon}
	\end{equation}
	onde $\epsilon = 10^{-12}$ evita divisões por zero. 
	
	Considerar corretas as implementações cuja diferença máxima seja menor que um limite aceitável (por exemplo, $10^{-8}$).
	
	\subsection{Descrição do hardware utilizado nos testes}
	
	Os testes foram realizados em uma máquina com as seguintes especificações:
	\begin{itemize}
		\item \textbf{Processador:} Ryzen 7 5700G (8 núcleos)
		\item \textbf{Memória RAM:} 32 GB
		\item \textbf{Sistema Operacional:} Windows 11 + WSL 2.4.10.0 (Ubuntu 24.04.2 LTS)
		\item \textbf{Compilador:} GCC 13.3.0
		\item \textbf{GPU}: 
	\end{itemize}
	
	\subsection{Métricas utilizadas para avaliação}
	
	Usaremos as seguintes métricas para definir e comparar as implementações:
	
	\subsubsection{Tempo de Execução}
	
	Medido em segundos, é o tempo total gasto para completar a multiplicação das matrizes. Não inclui o tempo de inicialização ou finalização do programa, apenas o tempo gasto na execução da função de multiplicação.
	\[
	T = T_{end} - T_{start}
	\]
	Onde:
	\begin{itemize}
		\item $T$ é o tempo de execução.
		\item $T_{start}$ é o tempo registrado no início da execução da função.
		\item $T_{end}$ é o tempo registrado ao final da execução da função.
	\end{itemize}
	
	O tempo total de execução será medido utilizando a função \texttt{DESCREVER FUNCAO AQUI...} para a versão MPI, e \texttt{DESCREVER FUNCAO AQUI....} para a versão CUDA e para a versão sequencial.
	
	Foi utilizado um script em python para automatizar a execução dos testes e coletar os tempos. O script executa cada versão múltiplas vezes com diferentes entradas para obter uma média dos tempos, calcula as métricas de desempenho e gera um relatório com os resultados, que foram utilizados para criar os gráficos apresentados na seção de resultados.
	
	\subsubsection{Speedup}
	Mede o ganho de desempenho da versão paralela em relação à sequencial.
	\[
	S_{p} = \frac{T_{s}}{T_{p}}
	\]
	Onde:
	\begin{itemize}
		\item $S_{p}$ é o speedup com $p$ processadores/threads.
		
		\item $T_{s}$ é o tempo de execução da versão sequencial.
		
		\item $T_{p}$ é o tempo de execução da versão paralela com $p$ processadores/threads.
	\end{itemize}
	
	\subsubsection{Eficiência}
	Mede quão bem os recursos de processamento estão sendo utilizados pela versão paralela. Onde 1 é a eficiência ideal (100\%).
	\[
	E_{p} = \frac{S_{p}}{p}= \frac{T_{s}}{p \cdot T_{p}}
	\]
	Onde:
	\begin{itemize}
		\item $E_{p}$ é a eficiência com $p$ processadores/threads. Varia entre 0 e 1.
		
		\item $S_{p}$ é o speedup com $p$ processadores/threads.
		
		\item $p$ é o número de processadores/threads.
		
		\item $T_{s}$ é o tempo de execução da versão sequencial.
		
		\item $T_{p}$ é o tempo de execução da versão paralela com $p$ processadores/threads.
	\end{itemize}
	
	\newpage
	\section{Resultados}\label{sec:resultados}
	
	Nesta seção, apresentamos os resultados obtidos a partir dos testes realizados nas implementações sequencial, paralela com CUDA da multiplicação de matrizes DGEMM. O resultado vai ser comparado com versões paralelizadas utilizando OpenMP e MPI\@ Os resultados incluem a validação de corretude, tabelas e gráficos de desempenho, bem como cálculos de \emph{speedup} e eficiência.
	
	\subsection{Tempo de execução}
	
	\begin{figure}[H]
		\centering
%		\includegraphics[width=0.9\textwidth]{img/execution-time-openmp.png}
		\caption{Tempo de execução (OpenMP)}\label{fig:tempo_execucao_omp}
	\end{figure}
	
	\begin{figure}[H]
		\centering
%		\includegraphics[width=0.9\textwidth]{img/execution-time-mpi.png}
		\caption{Tempo de execução (MPI)}\label{fig:tempo_execucao_mpi}
	\end{figure}
	\begin{figure}[H]
		\centering
		%		\includegraphics[width=0.9\textwidth]{img/execution-time-mpi.png}
		\caption{Tempo de execução (CUDA)}\label{fig:tempo_execucao_cuda}
	\end{figure}
	REFAZER: As Figuras~\ref{fig:tempo_execucao_omp} e~\ref{fig:tempo_execucao_mpi} mostram o tempo de execução das implementações com OpenMP e MPI, respectivamente, para diferentes tamanhos de matrizes e números de threads/processos. Observa-se que o tempo de execução diminui com o aumento do número de threads/processos, evidenciando o benefício do paralelismo. Contudo, é possível perceber que a implementação com OpenMP apresenta tempos de execução menores em comparação com a implementação com MPI para o mesmo número de threads/processos, o que pode ser atribuído à sobrecarga de comunicação inerente ao modelo de memória distribuída utilizado pelo MPI.\@
	
	\subsection{Speedup}
	
	\begin{figure}[H]
		\centering
%		\includegraphics[width=0.9\textwidth]{img/speedup-openmp.png}
		\caption{Speedup (OpenMP)}\label{fig:speedup_omp}
	\end{figure}
	
	\begin{figure}[H]
		\centering
	%	\includegraphics[width=0.9\textwidth]{img/speedup-mpi.png}
		\caption{Speedup (MPI)}\label{fig:speedup_mpi}
	\end{figure}
		\begin{figure}[H]
		\centering
		%	\includegraphics[width=0.9\textwidth]{img/speedup-mpi.png}
		\caption{Speedup (CUDA)}\label{fig:speedup_cuda}
	\end{figure}
	
	REFAZER: As Figuras~\ref{fig:speedup_omp} e~\ref{fig:speedup_mpi} apresentam o \emph{speedup} obtido pelas implementações com OpenMP e MPI, respectivamente. Observa-se que ambas as implementações conseguem alcançar um \emph{speedup} significativo com o aumento do número de threads/processos.
	
	\subsection{Eficiência}
	
	\begin{figure}[H]
		\centering
	%	\includegraphics[width=0.9\textwidth]{img/efficiency-openmp.png}
		\caption{Eficiência (OpenMP)}\label{fig:eficiencia_omp}
	\end{figure}
	
	\begin{figure}[H]
		\centering
	%	\includegraphics[width=0.9\textwidth]{img/efficiency-mpi.png}
		\caption{Eficiência (MPI)}\label{fig:eficiencia_mpi}
	\end{figure}
	\begin{figure}[H]
		\centering
		%	\includegraphics[width=0.9\textwidth]{img/efficiency-mpi.png}
		\caption{Eficiência (CUDA)}\label{fig:eficiencia_cuda}
	\end{figure}
	
	REFAZER: As Figuras~\ref{fig:eficiencia_omp} e~\ref{fig:eficiencia_mpi} mostram a eficiência das implementações com OpenMP e MPI, respectivamente. A eficiência tende a diminuir com o aumento do número de threads/processos, o que é esperado devido à sobrecarga de comunicação e sincronização. Não há uma diferença significativa na eficiência entre as duas implementações, embora a implementação com OpenMP apresente uma leve vantagem em alguns casos.
	
	\newpage
	\section{Discussão}\label{sec:discussao}
	
	Nesta seção, discutimos os resultados obtidos nas implementações sequencial, paralela com cuda da multiplicação de matrizes DGEMM.\@ Analisamos o desempenho, a escalabilidade e as limitações de cada abordagemx.
	
	\subsection{Análise de Desempenho}
	
	A implementação com OpenMP apresentou um desempenho superior em comparação com a implementação com MPI para o mesmo número de threads/processos. Isso pode ser atribuído à menor sobrecarga de comunicação no modelo de memória compartilhada utilizado pelo OpenMP, em contraste com o modelo de memória distribuída do MPI, que requer troca de mensagens entre processos.
	
	\subsection{Escalabilidade}
	
	Ambas as implementações demonstraram boa escalabilidade com o aumento do número de threads/processos. No entanto, a eficiência diminuiu à medida que o número de threads/processos aumentou, indicando que a sobrecarga de comunicação e sincronização começa a impactar negativamente o desempenho em configurações com muitos processos.
	
	Porém, a versão com MPI possui uma vantagem clara: sua capacidade de escalar para um número maior de processos em ambientes distribuídos, como clusters de computadores, onde o OpenMP não é aplicável. Isso torna a implementação com MPI mais adequada para aplicações que exigem alta escalabilidade e distribuição de carga em múltiplos nós.
	
	\subsection{Limitações}
	
	Uma limitação observada na implementação com MPI é a complexidade adicional na gestão da comunicação entre processos, o que levou a diversos erros e dificuldades no debug. Além disso, a sobrecarga de comunicação pode se tornar um gargalo em sistemas com alta latência ou largura de banda limitada.
	
	\newpage
	\section{Conclusão}\label{sec:conclusao}
	
	Neste relatório, apresentamos a implementação e análise de desempenho da multiplicação de matrizes DGEMM em suas versões sequencial, paralela com OpenMP e paralela com MPI.A implementação com OpenMP demonstrou um desempenho superior em ambientes de memória compartilhada, enquanto a implementação com MPI destacou-se pela sua capacidade de escalar em ambientes distribuídos. Ambas as implementações apresentaram boa escalabilidade, embora a eficiência tenha diminuído com o aumento do número de threads/processos devido à sobrecarga de comunicação.
	
	Em trabalhos futuros, sugerimos explorar otimizações adicionais na implementação com MPI, como o uso de técnicas avançadas de comunicação e balanceamento de carga. Além disso, seria interessante investigar a combinação de OpenMP e MPI para aproveitar os benefícios de ambos os modelos de paralelismo em sistemas híbridos.
	
	\newpage
	\begin{thebibliography}{9}
		\raggedright{}
		\bibitem{latexcompanion} Orellana, E., \texttt{Materiais de slides vistos em aula}
		
		\bibitem{latexcompanion} OpenMP, Disponível em: \url{https://www.openmp.org/wp-content/uploads/OpenMP-RefGuide-6.0-OMP60SC24-web.pdf}. Acesso em: 22 de Setembro de 2025.
		
		\bibitem{latexcompanion} Open MPI, Disponível em: \url{https://www.open-mpi.org/doc/v4.1/}. Acesso em: 11 de Novembro de 2025.
		
		\bibitem{knuthwebsite} Brasil Escola, Disponível em: \url{https://brasilescola.uol.com.br/matematica/multiplicacao-matrizes.htm}. Acesso em: 22 de Setembro de 2025.
		
		\bibitem{knuthwebsite} VSP-BERLIN, Disponível em: \url{https://svn.vsp.tu-berlin.de/repos/public-svn/publications/kn-old/strc/html/node9.html}. Acesso em: 23 de Setembro de 2025.
		
		\bibitem{knuthwebsite} Wikipedia, Disponível em: \url{https://en.wikipedia.org/wiki/Loop\_nest\_optimization}. Acesso em: 23 de Setembro de 2025.
	\end{thebibliography}
	
\end{document}
